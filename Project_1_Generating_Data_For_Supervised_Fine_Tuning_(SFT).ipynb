{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhuvana-ak/uplimit-synthetic_data_gen/blob/main/Project_1_Generating_Data_For_Supervised_Fine_Tuning_(SFT).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project 1: Generating Data For Supervised Fine-Tuning (SFT)\n",
        "\n",
        "\n",
        "First, install the required dependencies."
      ],
      "metadata": {
        "id": "UU1MpxWjAE7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"distilabel[hf-inference-endpoints]\" \"huggingface-hub\" \"model2vec\" \"semhash\" -U -q"
      ],
      "metadata": {
        "id": "egDDiNytBdNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we can log in to the Hugging Face Hub and configure our token through the login method."
      ],
      "metadata": {
        "id": "kp5_3rNZCPPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ],
      "metadata": {
        "id": "qlQkpuL7BoP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Generate a dataset for instruction tuning\n",
        "\n",
        "### Basic dataset"
      ],
      "metadata": {
        "id": "lqQrFP5gBckm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrzO6AlXACaG"
      },
      "outputs": [],
      "source": [
        "from distilabel.pipeline import Pipeline\n",
        "from distilabel.steps import LoadDataFromDicts\n",
        "from distilabel.steps.tasks import TextGeneration\n",
        "from distilabel.models.llms.huggingface import InferenceEndpointsLLM\n",
        "\n",
        "with Pipeline() as pipeline:\n",
        "    data = LoadDataFromDicts(data=[{\"instruction\": \"Generate a short question about Uplimit.\"}])\n",
        "    llm = InferenceEndpointsLLM(base_url=\"https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-3B-Instruct\")\n",
        "    gen_a = TextGeneration(llm=llm, output_mappings={\"generation\": \"instruction\"})\n",
        "    gen_b = TextGeneration(llm=llm, output_mappings={\"generation\": \"response\"})\n",
        "    data >> gen_a >> gen_b\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    username = \"uplimit\"\n",
        "    distiset = pipeline.run(use_cache=False)\n",
        "    distiset.push_to_hub(f\"{username}/uplimit-synthetic-data-week-1-basic\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With seed data\n"
      ],
      "metadata": {
        "id": "JyDWvJR0C2LV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from distilabel.models.llms.huggingface import InferenceEndpointsLLM\n",
        "from distilabel.pipeline import Pipeline\n",
        "from distilabel.steps import LoadDataFromHub\n",
        "from distilabel.steps.tasks import SelfInstruct, TextGeneration\n",
        "\n",
        "with Pipeline() as pipeline:\n",
        "    data = LoadDataFromHub(repo_id=\"dvilasuero/finepersonas-v0.1-tiny\", num_examples=1)\n",
        "    llm = InferenceEndpointsLLM(\n",
        "        base_url=\"https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    )\n",
        "    gen_a = SelfInstruct(llm=llm, input_mappings={\"input\": \"persona\"})\n",
        "    gen_b = TextGeneration(llm=llm, input_mappings={\"instruction\": \"instructions\"})\n",
        "    data >> gen_a >> gen_b\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    username = \"uplimit\"\n",
        "    distiset = pipeline.run(use_cache=False)\n",
        "    distiset.push_to_hub(\n",
        "        f\"{username}/uplimit-synthetic-data-week-1-with-seed\", include_script=True\n",
        "    )"
      ],
      "metadata": {
        "id": "dVeV7su9DLOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With instruction evolution\n"
      ],
      "metadata": {
        "id": "454TbRqIDNq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from distilabel.models.llms.huggingface import InferenceEndpointsLLM\n",
        "from distilabel.pipeline import Pipeline\n",
        "from distilabel.steps import LoadDataFromHub\n",
        "from distilabel.steps.tasks import EvolInstruct, SelfInstruct, TextGeneration\n",
        "\n",
        "with Pipeline() as pipeline:\n",
        "    data = LoadDataFromHub(repo_id=\"dvilasuero/finepersonas-v0.1-tiny\", num_examples=1)\n",
        "    llm = InferenceEndpointsLLM(\n",
        "        base_url=\"https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    )\n",
        "    gen_a = SelfInstruct(llm=llm, input_mappings={\"input\": \"persona\"})\n",
        "    gen_b = EvolInstruct(\n",
        "        llm=llm, num_evolutions=1, input_mappings={\"instruction\": \"instructions\"}\n",
        "    )\n",
        "    gen_c = TextGeneration(\n",
        "        llm=llm, input_mappings={\"instruction\": \"evolved_instruction\"}\n",
        "    )\n",
        "    data >> gen_a >> gen_b >> gen_c\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    distiset = pipeline.run(use_cache=False)\n",
        "    distiset.push_to_hub(\n",
        "        \"uplimit/uplimit-synthetic-data-week-1-with-evol\",\n",
        "        include_script=True,\n",
        "    )"
      ],
      "metadata": {
        "id": "HHw4yIY1DOTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With multi-turn conversations"
      ],
      "metadata": {
        "id": "2vgBdQ8iDOmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from distilabel.models.llms.huggingface import InferenceEndpointsLLM\n",
        "from distilabel.pipeline import Pipeline\n",
        "from distilabel.steps.tasks import MagpieGenerator\n",
        "\n",
        "with Pipeline() as pipeline:\n",
        "    llm = InferenceEndpointsLLM(\n",
        "        base_url=\"https://api-inference.huggingface.co/models/meta-llama/Llama-3.2-3B-Instruct\",\n",
        "        tokenizer_id=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "        magpie_pre_query_template=\"llama3\",\n",
        "        use_magpie_template=True,\n",
        "    )\n",
        "\n",
        "    gen_a = MagpieGenerator(llm=llm, n_turns=2, num_rows=1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    distiset = pipeline.run(use_cache=False)\n",
        "    distiset.push_to_hub(\n",
        "        \"uplimit/uplimit-synthetic-data-week-1-with-multi-turn\",\n",
        "        include_script=True,\n",
        "    )"
      ],
      "metadata": {
        "id": "s0oUL7K5DRpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2. Explore and evaluate the generated data\n",
        "\n",
        "“Look at  your data!” is a phrase that often comes by a lot during the various phases of AI development. It is important to be able to understand where your data performs well and where it might be improved. You can consider feature engineering through basic heuristics or through more advanced methods like entropy, embeddings or classification models.\n",
        "\n",
        "There are many interesting resources to do data exploration and cleaning with but we will name a couple to get you started.\n",
        "\n",
        "The Dataset Tools organisation on Hugging Face hold collections of tools and models to explore data or do feature engineering, like a domain classifier, a really fast embedder, and a toxicity classifier.\n",
        "\n",
        "There is an integration with Nomic AI that allows you to Explore, Curate and Vector Search Any Hugging Face Dataset with Nomic Atlas\n",
        "\n",
        "Text Descriptives is a Python library for calculating a large variety of metrics from text.\n",
        "\n",
        "Explore and potentially filter your generated dataset. Report on this analysis within your published dataset by updating the dataset and by adding scripts, written text, photos or videos.\n"
      ],
      "metadata": {
        "id": "U_mnxIsoBEzK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first use semhash to deduplicate records based on semantic similarity."
      ],
      "metadata": {
        "id": "0lZ4yqlklqXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from semhash import SemHash\n",
        "from transformers import pipeline\n",
        "\n",
        "\n",
        "ds = load_dataset(path=\"argilla/distilabel-intel-orca-dpo-pairs\", split=\"train\")\n",
        "\n",
        "semhash = SemHash.from_records(records=ds[\"input\"])\n",
        "\n",
        "# Deduplicate the texts\n",
        "deduplicated_texts = semhash.self_deduplicate(threshold=0.8).deduplicated\n",
        "print(f\"Original dataset: {len(ds)}. Filtered dataset: {len(deduplicated_texts)}. Percentage left: {len(deduplicated_texts)/len(ds)}\")"
      ],
      "metadata": {
        "id": "aHIx7WQSln-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can add a score to each of the text based on their educational value."
      ],
      "metadata": {
        "id": "Egg5Cia5lvvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\n",
        "    task=\"text-classification\",\n",
        "    model=\"HuggingFaceFW/fineweb-edu-classifier\"\n",
        ")\n",
        "\n",
        "quality_predictions = pipe(deduplicated_texts, truncation=True, verbose=True)"
      ],
      "metadata": {
        "id": "62-4nzzom9nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can filter based on a minimum score and certain percentage of records."
      ],
      "metadata": {
        "id": "PVv965WJnCed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "quality_scores = [i[\"score\"] for i in quality_predictions]\n",
        "\n",
        "df = pd.DataFrame.from_dict(\n",
        "    {\n",
        "        \"text\": deduplicated_texts,\n",
        "        \"quality\": quality_scores\n",
        "    }\n",
        ")\n",
        "p_to_keep = 0.8\n",
        "min_score = 0.8\n",
        "df.sort_values(by=\"quality\", ascending=False, inplace=True)\n",
        "df = df.head(int(len(df)*p_to_keep))\n",
        "df = df[df[\"quality\"] > min_score]\n",
        "print(f\"Original dataset: {len(ds)}. Filtered dataset: {len(df)}. Percentage left: {len(df)/len(ds)}\")"
      ],
      "metadata": {
        "id": "A1LF1btUnG7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Publish the dataset on Hugging Face\n",
        "\n",
        "Publish the dataset on the Hugging Face Hub. Make sure you update the ModelCard with all relevant information. Additionally, link the exploratory data analysis and preferably also add an analysis script."
      ],
      "metadata": {
        "id": "ayKd31oDllbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "ds = Dataset.from_pandas(df, preserve_index=False)\n",
        "ds = ds.push_to_hub(\"uplimit/uplimit-synthetic-data-week-1-filtered\")"
      ],
      "metadata": {
        "id": "59NuANOEoPSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can go to our dataset page and update the ModelCard with all relevant information. Additionally, we can add an analysis script."
      ],
      "metadata": {
        "id": "9D1eQ-1eoPxP"
      }
    }
  ]
}